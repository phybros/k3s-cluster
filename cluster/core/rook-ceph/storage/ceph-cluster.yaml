kind: ConfigMap
apiVersion: v1
metadata:
  name: rook-config-override
  namespace: rook-ceph # namespace:cluster
data:
  config: |
    [global]
    osd_pool_default_size = 2
    [mon]
    mon_data_avail_warn = 10

---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.7
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: true
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  removeOSDsIfOutAndSafeToRemove: false
  mon:
    count: 2
    allowMultiplePerNode: false
  mgr:
    count: 1
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true
  placement:
    all:
      tolerations:
        - operator: "Exists"
  monitoring:
    enabled: true
  network:
  crashCollector:
    disable: false
  cleanupPolicy:
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
  dashboard:
    enabled: true
    port: 7000
    ssl: false
  disruptionManagement:
    managePodBudgets: false
    osdMaintenanceTimeout: 30
    manageMachineDisruptionBudgets: false
    machineDisruptionBudgetNamespace: openshift-machine-api
  resources:
    mon:
      requests:
        cpu: 35m
        memory: 800Mi
      limits:
        memory: 1024Mi
    osd:
      requests:
        cpu: 35m
        memory: 2048Mi
      limits:
        memory: 4096Mi
  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "1"
    nodes:
      - name: "whale"
        devices:
          - name: "/dev/disk/by-id/ata-KINGSTON_SA400S37240G_50026B77841391BF"
      - name: "duck"
        devices:
          - name: "/dev/disk/by-id/ata-KINGSTON_SA400S37240G_50026B7784139094"
